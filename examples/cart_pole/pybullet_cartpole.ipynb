{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/pybullet_envs/bullet\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#rendersで画面を表示するか決める\n",
    "#1番初めにやらないとエラー出る\n",
    "from pybullet_envs.bullet.cartpole_bullet import CartPoleBulletEnv\n",
    "env = CartPoleBulletEnv(renders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from baselines import deepq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback(lcl, glb):\n",
    "    # stop training if reward exceeds 199\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 199\n",
    "    return is_solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = deepq.models.mlp([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 38       |\n",
      "| steps                   | 341      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 36.3     |\n",
      "| steps                   | 688      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 34.7     |\n",
      "| steps                   | 1006     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 34.3     |\n",
      "| steps                   | 1336     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 39.6     |\n",
      "| steps                   | 1941     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 77       |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 38.5     |\n",
      "| steps                   | 2268     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 74       |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 37.3     |\n",
      "| steps                   | 2573     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 70       |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 37.8     |\n",
      "| steps                   | 2989     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 67       |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 37.8     |\n",
      "| steps                   | 3367     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 36.9     |\n",
      "| steps                   | 3652     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | 36.4     |\n",
      "| steps                   | 3978     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | 35.6     |\n",
      "| steps                   | 4247     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 54       |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | 36.1     |\n",
      "| steps                   | 4618     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 52       |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | 35.5     |\n",
      "| steps                   | 4888     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 48       |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | 33.4     |\n",
      "| steps                   | 5281     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | 33.3     |\n",
      "| steps                   | 5602     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 40       |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | 35.4     |\n",
      "| steps                   | 6109     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 36       |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | 34.5     |\n",
      "| steps                   | 6436     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 32       |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | 35.2     |\n",
      "| steps                   | 6887     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 37.6     |\n",
      "| steps                   | 7409     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 21       |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | 40.2     |\n",
      "| steps                   | 7999     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 15       |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | 43.9     |\n",
      "| steps                   | 8641     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | 48.7     |\n",
      "| steps                   | 9491     |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> 51.1\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | 57.2     |\n",
      "| steps                   | 10613    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | 68.2     |\n",
      "| steps                   | 12103    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 260      |\n",
      "| mean 100 episode reward | 78.1     |\n",
      "| steps                   | 13410    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 270      |\n",
      "| mean 100 episode reward | 87.5     |\n",
      "| steps                   | 14859    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 280      |\n",
      "| mean 100 episode reward | 103      |\n",
      "| steps                   | 16703    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 290      |\n",
      "| mean 100 episode reward | 114      |\n",
      "| steps                   | 18336    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 120      |\n",
      "| steps                   | 19361    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 51.1 -> 120.5\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 310      |\n",
      "| mean 100 episode reward | 123      |\n",
      "| steps                   | 20312    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 320      |\n",
      "| mean 100 episode reward | 128      |\n",
      "| steps                   | 21469    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 330      |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 22783    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 340      |\n",
      "| mean 100 episode reward | 131      |\n",
      "| steps                   | 23688    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | 125      |\n",
      "| steps                   | 24586    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 360      |\n",
      "| mean 100 episode reward | 122      |\n",
      "| steps                   | 25557    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 370      |\n",
      "| mean 100 episode reward | 118      |\n",
      "| steps                   | 26652    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 380      |\n",
      "| mean 100 episode reward | 107      |\n",
      "| steps                   | 27402    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 390      |\n",
      "| mean 100 episode reward | 98.6     |\n",
      "| steps                   | 28197    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 96       |\n",
      "| steps                   | 28963    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 410      |\n",
      "| mean 100 episode reward | 92.4     |\n",
      "| steps                   | 29550    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 420      |\n",
      "| mean 100 episode reward | 86.7     |\n",
      "| steps                   | 30135    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 430      |\n",
      "| mean 100 episode reward | 79.3     |\n",
      "| steps                   | 30716    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 440      |\n",
      "| mean 100 episode reward | 75.2     |\n",
      "| steps                   | 31208    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | 70.9     |\n",
      "| steps                   | 31673    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 460      |\n",
      "| mean 100 episode reward | 64.8     |\n",
      "| steps                   | 32039    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 470      |\n",
      "| mean 100 episode reward | 57.5     |\n",
      "| steps                   | 32404    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 480      |\n",
      "| mean 100 episode reward | 54.6     |\n",
      "| steps                   | 32858    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 490      |\n",
      "| mean 100 episode reward | 50.6     |\n",
      "| steps                   | 33260    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 47.5     |\n",
      "| steps                   | 33713    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 510      |\n",
      "| mean 100 episode reward | 45.7     |\n",
      "| steps                   | 34116    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 520      |\n",
      "| mean 100 episode reward | 44.5     |\n",
      "| steps                   | 34585    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 530      |\n",
      "| mean 100 episode reward | 42.7     |\n",
      "| steps                   | 34986    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 540      |\n",
      "| mean 100 episode reward | 41.1     |\n",
      "| steps                   | 35321    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | 39.7     |\n",
      "| steps                   | 35639    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 560      |\n",
      "| mean 100 episode reward | 39.1     |\n",
      "| steps                   | 35950    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 570      |\n",
      "| mean 100 episode reward | 38.1     |\n",
      "| steps                   | 36214    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 580      |\n",
      "| mean 100 episode reward | 36.2     |\n",
      "| steps                   | 36475    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 590      |\n",
      "| mean 100 episode reward | 35.2     |\n",
      "| steps                   | 36779    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 33.2     |\n",
      "| steps                   | 37038    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 610      |\n",
      "| mean 100 episode reward | 31.2     |\n",
      "| steps                   | 37240    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 620      |\n",
      "| mean 100 episode reward | 29.3     |\n",
      "| steps                   | 37511    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 630      |\n",
      "| mean 100 episode reward | 27.4     |\n",
      "| steps                   | 37725    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 640      |\n",
      "| mean 100 episode reward | 26.1     |\n",
      "| steps                   | 37935    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 650      |\n",
      "| mean 100 episode reward | 25.1     |\n",
      "| steps                   | 38145    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 660      |\n",
      "| mean 100 episode reward | 23.8     |\n",
      "| steps                   | 38328    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 670      |\n",
      "| mean 100 episode reward | 23.2     |\n",
      "| steps                   | 38538    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 680      |\n",
      "| mean 100 episode reward | 22.6     |\n",
      "| steps                   | 38740    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 690      |\n",
      "| mean 100 episode reward | 21.9     |\n",
      "| steps                   | 38970    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 21.2     |\n",
      "| steps                   | 39161    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 710      |\n",
      "| mean 100 episode reward | 21       |\n",
      "| steps                   | 39339    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 720      |\n",
      "| mean 100 episode reward | 20       |\n",
      "| steps                   | 39511    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 730      |\n",
      "| mean 100 episode reward | 19.6     |\n",
      "| steps                   | 39686    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 740      |\n",
      "| mean 100 episode reward | 19.5     |\n",
      "| steps                   | 39887    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 750      |\n",
      "| mean 100 episode reward | 19.2     |\n",
      "| steps                   | 40063    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 760      |\n",
      "| mean 100 episode reward | 19       |\n",
      "| steps                   | 40229    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 770      |\n",
      "| mean 100 episode reward | 19       |\n",
      "| steps                   | 40443    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 780      |\n",
      "| mean 100 episode reward | 19       |\n",
      "| steps                   | 40644    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 790      |\n",
      "| mean 100 episode reward | 18.3     |\n",
      "| steps                   | 40802    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 18.5     |\n",
      "| steps                   | 41008    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 810      |\n",
      "| mean 100 episode reward | 18.5     |\n",
      "| steps                   | 41186    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 820      |\n",
      "| mean 100 episode reward | 18.5     |\n",
      "| steps                   | 41361    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 830      |\n",
      "| mean 100 episode reward | 18.8     |\n",
      "| steps                   | 41570    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 840      |\n",
      "| mean 100 episode reward | 18.7     |\n",
      "| steps                   | 41758    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 850      |\n",
      "| mean 100 episode reward | 18.8     |\n",
      "| steps                   | 41941    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 860      |\n",
      "| mean 100 episode reward | 19.6     |\n",
      "| steps                   | 42190    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 870      |\n",
      "| mean 100 episode reward | 20.4     |\n",
      "| steps                   | 42485    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 880      |\n",
      "| mean 100 episode reward | 20.7     |\n",
      "| steps                   | 42715    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 890      |\n",
      "| mean 100 episode reward | 21.2     |\n",
      "| steps                   | 42926    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 21.6     |\n",
      "| steps                   | 43163    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 910      |\n",
      "| mean 100 episode reward | 22.1     |\n",
      "| steps                   | 43400    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 920      |\n",
      "| mean 100 episode reward | 23       |\n",
      "| steps                   | 43657    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 930      |\n",
      "| mean 100 episode reward | 24       |\n",
      "| steps                   | 43970    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 940      |\n",
      "| mean 100 episode reward | 26.2     |\n",
      "| steps                   | 44382    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 950      |\n",
      "| mean 100 episode reward | 27.5     |\n",
      "| steps                   | 44687    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 960      |\n",
      "| mean 100 episode reward | 27.7     |\n",
      "| steps                   | 44957    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 970      |\n",
      "| mean 100 episode reward | 28.1     |\n",
      "| steps                   | 45297    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 980      |\n",
      "| mean 100 episode reward | 30.9     |\n",
      "| steps                   | 45807    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 990      |\n",
      "| mean 100 episode reward | 34       |\n",
      "| steps                   | 46323    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 37.4     |\n",
      "| steps                   | 46899    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1010     |\n",
      "| mean 100 episode reward | 44       |\n",
      "| steps                   | 47797    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1020     |\n",
      "| mean 100 episode reward | 50       |\n",
      "| steps                   | 48661    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1030     |\n",
      "| mean 100 episode reward | 55.5     |\n",
      "| steps                   | 49521    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1040     |\n",
      "| mean 100 episode reward | 57.9     |\n",
      "| steps                   | 50173    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1050     |\n",
      "| mean 100 episode reward | 64.6     |\n",
      "| steps                   | 51145    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1060     |\n",
      "| mean 100 episode reward | 72.6     |\n",
      "| steps                   | 52215    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1070     |\n",
      "| mean 100 episode reward | 78.2     |\n",
      "| steps                   | 53119    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1080     |\n",
      "| mean 100 episode reward | 82.6     |\n",
      "| steps                   | 54069    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1090     |\n",
      "| mean 100 episode reward | 90.4     |\n",
      "| steps                   | 55361    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 97.6     |\n",
      "| steps                   | 56664    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1110     |\n",
      "| mean 100 episode reward | 101      |\n",
      "| steps                   | 57875    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1120     |\n",
      "| mean 100 episode reward | 107      |\n",
      "| steps                   | 59372    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1130     |\n",
      "| mean 100 episode reward | 118      |\n",
      "| steps                   | 61374    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1140     |\n",
      "| mean 100 episode reward | 132      |\n",
      "| steps                   | 63402    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1150     |\n",
      "| mean 100 episode reward | 135      |\n",
      "| steps                   | 64631    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1160     |\n",
      "| mean 100 episode reward | 140      |\n",
      "| steps                   | 66185    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1170     |\n",
      "| mean 100 episode reward | 145      |\n",
      "| steps                   | 67605    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1180     |\n",
      "| mean 100 episode reward | 145      |\n",
      "| steps                   | 68597    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1190     |\n",
      "| mean 100 episode reward | 136      |\n",
      "| steps                   | 68923    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 120.5 -> 136.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 138      |\n",
      "| steps                   | 70410    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1210     |\n",
      "| mean 100 episode reward | 142      |\n",
      "| steps                   | 72082    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1220     |\n",
      "| mean 100 episode reward | 141      |\n",
      "| steps                   | 73502    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1230     |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 74715    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1240     |\n",
      "| mean 100 episode reward | 127      |\n",
      "| steps                   | 76067    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1250     |\n",
      "| mean 100 episode reward | 141      |\n",
      "| steps                   | 78735    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 136.6 -> 145.1\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1260     |\n",
      "| mean 100 episode reward | 150      |\n",
      "| steps                   | 81237    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1270     |\n",
      "| mean 100 episode reward | 165      |\n",
      "| steps                   | 84122    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1280     |\n",
      "| mean 100 episode reward | 173      |\n",
      "| steps                   | 85933    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1290     |\n",
      "| mean 100 episode reward | 193      |\n",
      "| steps                   | 88215    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 187      |\n",
      "| steps                   | 89075    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1310     |\n",
      "| mean 100 episode reward | 178      |\n",
      "| steps                   | 89828    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 145.1 -> 175.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1320     |\n",
      "| mean 100 episode reward | 169      |\n",
      "| steps                   | 90398    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1330     |\n",
      "| mean 100 episode reward | 164      |\n",
      "| steps                   | 91112    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1340     |\n",
      "| mean 100 episode reward | 156      |\n",
      "| steps                   | 91628    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1350     |\n",
      "| mean 100 episode reward | 136      |\n",
      "| steps                   | 92281    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1360     |\n",
      "| mean 100 episode reward | 113      |\n",
      "| steps                   | 92542    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1370     |\n",
      "| mean 100 episode reward | 86.8     |\n",
      "| steps                   | 92800    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1380     |\n",
      "| mean 100 episode reward | 72.7     |\n",
      "| steps                   | 93202    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1390     |\n",
      "| mean 100 episode reward | 51.8     |\n",
      "| steps                   | 93391    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 44.8     |\n",
      "| steps                   | 93552    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1410     |\n",
      "| mean 100 episode reward | 38.7     |\n",
      "| steps                   | 93696    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1420     |\n",
      "| mean 100 episode reward | 34.4     |\n",
      "| steps                   | 93839    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1430     |\n",
      "| mean 100 episode reward | 28.6     |\n",
      "| steps                   | 93968    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1440     |\n",
      "| mean 100 episode reward | 24.8     |\n",
      "| steps                   | 94108    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1450     |\n",
      "| mean 100 episode reward | 19.7     |\n",
      "| steps                   | 94248    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1460     |\n",
      "| mean 100 episode reward | 18.4     |\n",
      "| steps                   | 94381    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1470     |\n",
      "| mean 100 episode reward | 17.1     |\n",
      "| steps                   | 94507    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1480     |\n",
      "| mean 100 episode reward | 14.4     |\n",
      "| steps                   | 94645    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1490     |\n",
      "| mean 100 episode reward | 13.7     |\n",
      "| steps                   | 94764    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 13.5     |\n",
      "| steps                   | 94898    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1510     |\n",
      "| mean 100 episode reward | 13.3     |\n",
      "| steps                   | 95026    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1520     |\n",
      "| mean 100 episode reward | 13.1     |\n",
      "| steps                   | 95150    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1530     |\n",
      "| mean 100 episode reward | 13       |\n",
      "| steps                   | 95268    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1540     |\n",
      "| mean 100 episode reward | 12.8     |\n",
      "| steps                   | 95386    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1550     |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 95507    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1560     |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 95636    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1570     |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 95765    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1580     |\n",
      "| mean 100 episode reward | 12.5     |\n",
      "| steps                   | 95894    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1590     |\n",
      "| mean 100 episode reward | 12.5     |\n",
      "| steps                   | 96012    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 12.4     |\n",
      "| steps                   | 96140    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1610     |\n",
      "| mean 100 episode reward | 12.5     |\n",
      "| steps                   | 96276    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1620     |\n",
      "| mean 100 episode reward | 12.5     |\n",
      "| steps                   | 96400    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1630     |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 96526    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1640     |\n",
      "| mean 100 episode reward | 12.7     |\n",
      "| steps                   | 96653    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1650     |\n",
      "| mean 100 episode reward | 12.7     |\n",
      "| steps                   | 96775    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1660     |\n",
      "| mean 100 episode reward | 12.7     |\n",
      "| steps                   | 96902    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1670     |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 97026    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1680     |\n",
      "| mean 100 episode reward | 12.4     |\n",
      "| steps                   | 97138    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1690     |\n",
      "| mean 100 episode reward | 12.5     |\n",
      "| steps                   | 97263    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 12.4     |\n",
      "| steps                   | 97385    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1710     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 97500    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1720     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 97623    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1730     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 97751    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1740     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 97873    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1750     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 97996    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1760     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98125    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1770     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98245    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1780     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98360    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1790     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98485    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98604    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1810     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98723    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1820     |\n",
      "| mean 100 episode reward | 12.3     |\n",
      "| steps                   | 98851    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1830     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 98971    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1840     |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 99094    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1850     |\n",
      "| mean 100 episode reward | 12.1     |\n",
      "| steps                   | 99209    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1860     |\n",
      "| mean 100 episode reward | 12       |\n",
      "| steps                   | 99322    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1870     |\n",
      "| mean 100 episode reward | 11.9     |\n",
      "| steps                   | 99436    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1880     |\n",
      "| mean 100 episode reward | 11.9     |\n",
      "| steps                   | 99552    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1890     |\n",
      "| mean 100 episode reward | 12       |\n",
      "| steps                   | 99681    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 11.9     |\n",
      "| steps                   | 99794    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1910     |\n",
      "| mean 100 episode reward | 11.9     |\n",
      "| steps                   | 99912    |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: 175.0\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprckwmg49/model\n",
      "Saving model to cartpole_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "act = deepq.learn(\n",
    "    env,\n",
    "    q_func=model,\n",
    "    lr=1e-3,\n",
    "    max_timesteps=100000,\n",
    "    buffer_size=50000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    print_freq=10,\n",
    "    callback=callback\n",
    ")\n",
    "print(\"Saving model to cartpole_model.pkl\")\n",
    "act.save(\"cartpole_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#学習済みを呼び出すとき\n",
    "#別にgym.makeは必要ないっぽい\n",
    "#てかこれダメ。他のモデルはmakeしないで毎回呼び出してた\n",
    "#env = gym.make('CartPoleBulletEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/pybullet_envs/bullet\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmplxw3tvw8/model\n"
     ]
    }
   ],
   "source": [
    "#訓練データの呼び出し\n",
    "# from pybullet_envs.bullet.cartpole_bullet import CartPoleBulletEnv\n",
    "# env = CartPoleBulletEnv(renders=True)\n",
    "# import gym\n",
    "# from baselines import deepq\n",
    "act = deepq.load(\"cartpole_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===episode: 0 =========================\n",
      "obs: [-0.15453655  0.         -0.0249039   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 112.0\n",
      "===episode: 1 =========================\n",
      "obs: [-0.31722248  0.          0.17811124  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 110.0\n",
      "===episode: 2 =========================\n",
      "obs: [-0.37841295  0.         -0.40733479  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 144.0\n",
      "===episode: 3 =========================\n",
      "obs: [ 0.22797737  0.         -0.1761826   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 104.0\n",
      "===episode: 4 =========================\n",
      "obs: [-0.10404442  0.          0.00585061  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 69.0\n",
      "===episode: 5 =========================\n",
      "obs: [ 0.37922846  0.         -0.01161936  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 35.0\n",
      "===episode: 6 =========================\n",
      "obs: [-0.02054696  0.          0.28158176  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 69.0\n",
      "===episode: 7 =========================\n",
      "obs: [-0.36096035  0.          0.46452763  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 31.0\n",
      "===episode: 8 =========================\n",
      "obs: [ 0.27000627  0.         -0.32519603  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 77.0\n",
      "===episode: 9 =========================\n",
      "obs: [-0.42224795  0.         -0.23543364  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 140.0\n",
      "===episode: 10 =========================\n",
      "obs: [ 0.14251183  0.         -0.36999433  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 114.0\n",
      "===episode: 11 =========================\n",
      "obs: [-0.31260616  0.          0.097172    0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 122.0\n",
      "===episode: 12 =========================\n",
      "obs: [ 0.46499842  0.         -0.49519626  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 101.0\n",
      "===episode: 13 =========================\n",
      "obs: [-0.39540581  0.          0.4770722   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 32.0\n",
      "===episode: 14 =========================\n",
      "obs: [ 0.31887872  0.         -0.35608518  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 104.0\n",
      "===episode: 15 =========================\n",
      "obs: [-0.12722971  0.          0.46020106  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 57.0\n",
      "===episode: 16 =========================\n",
      "obs: [0.11817818 0.         0.30394935 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 13.0\n",
      "===episode: 17 =========================\n",
      "obs: [0.43526841 0.         0.08921566 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 27.0\n",
      "===episode: 18 =========================\n",
      "obs: [-0.19102937  0.         -0.10332035  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 113.0\n",
      "===episode: 19 =========================\n",
      "obs: [0.0033409  0.         0.14686096 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 46.0\n",
      "===episode: 20 =========================\n",
      "obs: [-0.25499554  0.         -0.19325953  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 136.0\n",
      "===episode: 21 =========================\n",
      "obs: [-0.37241883  0.         -0.39897035  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 142.0\n",
      "===episode: 22 =========================\n",
      "obs: [0.19745299 0.         0.20243762 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 14.0\n",
      "===episode: 23 =========================\n",
      "obs: [-0.15495268  0.         -0.14931856  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 128.0\n",
      "===episode: 24 =========================\n",
      "obs: [ 0.34599749  0.         -0.1017308   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 48.0\n",
      "===episode: 25 =========================\n",
      "obs: [ 0.23178351  0.         -0.39084014  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 115.0\n",
      "===episode: 26 =========================\n",
      "obs: [-0.32435415  0.          0.11602316  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 117.0\n",
      "===episode: 27 =========================\n",
      "obs: [-0.35945848  0.          0.30472559  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 76.0\n",
      "===episode: 28 =========================\n",
      "obs: [ 0.09874147  0.         -0.35228387  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 122.0\n",
      "===episode: 29 =========================\n",
      "obs: [-0.06781245  0.         -0.4789225   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 125.0\n",
      "===episode: 30 =========================\n",
      "obs: [-0.22565905  0.          0.23881723  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 70.0\n",
      "===episode: 31 =========================\n",
      "obs: [ 0.09750377  0.         -0.09302472  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 282.0\n",
      "===episode: 32 =========================\n",
      "obs: [ 0.24361298  0.         -0.01898574  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 16.0\n",
      "===episode: 33 =========================\n",
      "obs: [0.10979295 0.         0.11491358 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 52.0\n",
      "===episode: 34 =========================\n",
      "obs: [0.12116625 0.         0.05774352 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 51.0\n",
      "===episode: 35 =========================\n",
      "obs: [0.20488728 0.         0.31511468 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 14.0\n",
      "===episode: 36 =========================\n",
      "obs: [0.18079858 0.         0.29001275 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 14.0\n",
      "===episode: 37 =========================\n",
      "obs: [-0.02350363  0.         -0.21982024  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 122.0\n",
      "===episode: 38 =========================\n",
      "obs: [-0.43349067  0.         -0.27563044  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 147.0\n",
      "===episode: 39 =========================\n",
      "obs: [-0.38002487  0.          0.47959275  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 35.0\n",
      "===episode: 40 =========================\n",
      "obs: [ 0.09660981  0.         -0.4855381   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 119.0\n",
      "===episode: 41 =========================\n",
      "obs: [ 0.47721221  0.         -0.42091742  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 99.0\n",
      "===episode: 42 =========================\n",
      "obs: [0.30810984 0.         0.4881647  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 14.0\n",
      "===episode: 43 =========================\n",
      "obs: [ 0.32439041  0.         -0.2545438   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 96.0\n",
      "===episode: 44 =========================\n",
      "obs: [0.05182736 0.         0.22769963 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 51.0\n",
      "===episode: 45 =========================\n",
      "obs: [-0.48170193  0.         -0.49908135  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 160.0\n",
      "===episode: 46 =========================\n",
      "obs: [ 0.35304909  0.         -0.2281385   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 98.0\n",
      "===episode: 47 =========================\n",
      "obs: [0.01810901 0.         0.28818323 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 52.0\n",
      "===episode: 48 =========================\n",
      "obs: [-0.06569851  0.         -0.00925094  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 72.0\n",
      "===episode: 49 =========================\n",
      "obs: [ 0.08513979  0.         -0.12712676  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 75.0\n",
      "===episode: 50 =========================\n",
      "obs: [ 0.41376681  0.         -0.07935551  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 41.0\n",
      "===episode: 51 =========================\n",
      "obs: [0.22339343 0.         0.01332946 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 49.0\n",
      "===episode: 52 =========================\n",
      "obs: [-0.08567051  0.          0.40113649  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 72.0\n",
      "===episode: 53 =========================\n",
      "obs: [0.43873144 0.         0.06138637 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 28.0\n",
      "===episode: 54 =========================\n",
      "obs: [ 0.44722061  0.         -0.03251704  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 41.0\n",
      "===episode: 55 =========================\n",
      "obs: [-0.35140295  0.          0.23793455  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 302.0\n",
      "===episode: 56 =========================\n",
      "obs: [-0.20566367  0.          0.23554038  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 36.0\n",
      "===episode: 57 =========================\n",
      "obs: [-0.16684269  0.         -0.33266453  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 129.0\n",
      "===episode: 58 =========================\n",
      "obs: [-0.17598789  0.          0.47520028  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 45.0\n",
      "===episode: 59 =========================\n",
      "obs: [ 0.44379098  0.         -0.06813052  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 79.0\n",
      "===episode: 60 =========================\n",
      "obs: [-0.36670591  0.          0.39365661  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 70.0\n",
      "===episode: 61 =========================\n",
      "obs: [-0.35694207  0.         -0.25598678  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 109.0\n",
      "===episode: 62 =========================\n",
      "obs: [-0.10628217  0.         -0.40115946  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 129.0\n",
      "===episode: 63 =========================\n",
      "obs: [ 0.3571571   0.         -0.35072211  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 95.0\n",
      "===episode: 64 =========================\n",
      "obs: [ 0.19065552  0.         -0.48341373  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 115.0\n",
      "===episode: 65 =========================\n",
      "obs: [ 0.3382562  0.        -0.0083302  0.       ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 43.0\n",
      "===episode: 66 =========================\n",
      "obs: [ 0.18517356  0.         -0.222895    0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 107.0\n",
      "===episode: 67 =========================\n",
      "obs: [ 0.15656472  0.         -0.3048414   0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 128.0\n",
      "===episode: 68 =========================\n",
      "obs: [-0.08816201  0.         -0.46519954  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 132.0\n",
      "===episode: 69 =========================\n",
      "obs: [-0.26662689  0.         -0.33184117  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 137.0\n",
      "===episode: 70 =========================\n",
      "obs: [0.12125187 0.         0.05325058 0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 52.0\n",
      "===episode: 71 =========================\n",
      "obs: [ 0.37090812  0.         -0.33160501  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 97.0\n",
      "===episode: 72 =========================\n",
      "obs: [-0.02161402  0.         -0.05899321  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 306.0\n",
      "===episode: 73 =========================\n",
      "obs: [-0.21740432  0.          0.34290904  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n",
      "Episode reward: 58.0\n",
      "===episode: 74 =========================\n",
      "obs: [-0.26169994  0.         -0.42422464  0.        ]\n",
      "type(obs): <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-288a4648ff63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mepisode_rew\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode reward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deepstation/.pyenv/versions/anaconda3-4.3.0/envs/pybullet/lib/python3.6/site-packages/pybullet_envs/bullet/cartpole_bullet.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#    time.sleep(self.timeStep)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetJointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetJointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "#Test\n",
    "episodes = 100\n",
    "for episode in range(episodes):\n",
    "    print(\"===episode:\", episode,\"=========================\")\n",
    "    obs, done = env.reset(), False\n",
    "    print(\"obs:\",obs)\n",
    "    print(\"type(obs):\",type(obs))\n",
    "    episode_rew = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        o = obs[None]\n",
    "        aa = act(o)\n",
    "        a = aa[0]\n",
    "        obs, rew, done, _ = env.step(a)\n",
    "        episode_rew += rew\n",
    "    print(\"Episode reward:\", episode_rew)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
