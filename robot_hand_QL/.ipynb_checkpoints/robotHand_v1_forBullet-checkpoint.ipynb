{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from datetime import datetime  # 時刻を取得\n",
    "import pybullet as p\n",
    "import pybullet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定数の設定'''\n",
    "NUM_DIZITIZED = 6  # 各状態の離散値への分割数\n",
    "discount = 0.99  # 時間割引率\n",
    "lr = 0.5  # 学習係数\n",
    "MAX_STEPS = 30  # 1試行のstep数cartpoleは195steps立ち続ければ終わり\n",
    "NUM_EPISODES = 100000  # 最大試行回数\n",
    "AREA_THRESH = 80  # 赤色物体面積の閾値．0~100で規格化してある\n",
    "\n",
    "'''学習するときはFalse，学習済みのモデルを使用するときはTrue'''\n",
    "# 使うq_tableのファイル名を\"trained_q_table.npy\"とすること\n",
    "TEST_MODE = False\n",
    "'''追加学習するときはTrue'''\n",
    "ADD_TRAIN_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''CartPoleのエージェントクラスです、棒付き台車そのものになります'''\n",
    "\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_Q_function(self, observation, action, reward, observation_next):\n",
    "        '''Q関数の更新'''\n",
    "        self.brain.update_Q_table(observation, action, reward, observation_next)\n",
    "\n",
    "    def get_action(self, observation, step):\n",
    "        '''行動の決定'''\n",
    "        action = self.brain.decide_action(observation, step)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    '''エージェントが持つ脳となるクラスです、Q学習を実行します'''\n",
    "\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # ロボットハンドの取れる行動数(コマンド数)\n",
    "        if TEST_MODE or ADD_TRAIN_MODE:  # 保存したQ-tableを使用\n",
    "            self.q_table = np.load('trained_q_table.npy')\n",
    "        else:  # Qテーブルを作成。行数は状態を分割数^(4変数)にデジタル変換した値、列数は行動数を示す\n",
    "            self.q_table = np.random.uniform(low=0, high=1, size=(NUM_DIZITIZED**num_states, num_actions))\n",
    "\n",
    "    def bins(self, clip_min, clip_max, num):\n",
    "        '''観測した状態（連続値）を離散値にデジタル変換する閾値を求める'''\n",
    "        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "\n",
    "    def digitize_state(self, observation):\n",
    "        '''観測したobservation状態を、離散値に変換する'''\n",
    "        area_sum, area_v = observation\n",
    "        digitized = [\n",
    "            np.digitize(area_sum, bins=self.bins(0, 10.0, NUM_DIZITIZED)), #　面積の比率\n",
    "            np.digitize(area_v, bins=self.bins(-10.0, 10.0, NUM_DIZITIZED))\n",
    "        ]\n",
    "        return sum([x * (NUM_DIZITIZED**i) for i, x in enumerate(digitized)]) #　6進数で表して計算を圧縮\n",
    "    \n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        '''QテーブルをQ学習により更新'''\n",
    "        state = self.digitize_state(observation)  # 状態を離散化\n",
    "        state_next = self.digitize_state(observation_next)  # 次の状態を離散化\n",
    "        Max_Q_next = max(self.q_table[state_next][:])\n",
    "        self.q_table[state, action] = self.q_table[state, action] + lr * (reward + discount * Max_Q_next - self.q_table[state, action])\n",
    "        self.save_Q_table()  # Q-tableを更新するたびに保存\n",
    "\n",
    "    def save_Q_table(self):\n",
    "        '''学習したQ-tableを保存'''\n",
    "        np.save(datetime.today().strftime(\"%m%d\")+'_q_table', self.q_table)\n",
    "\n",
    "    def load_Q_table(self):  # NoneTypeで読み込んでしまうため使ってない\n",
    "        '''学習済みのQ-tableを読み込み'''\n",
    "        np.load('trained_q_table.npy')\n",
    "\n",
    "    def decide_action(self, observation, episode):\n",
    "        '''ε-greedy法で徐々に最適行動のみを採用する'''\n",
    "        state = self.digitize_state(observation)\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action = np.random.choice(self.num_actions)  # 0,1の行動をランダムに返す\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  Environment:\n",
    "    '''CartPoleを実行する環境のクラスです'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_states = 2  # 課題の状態の数(面積と重心(x,y)と、それぞれの変化量で6つ)\n",
    "        self.num_actions = 4  # ロボットハンドの行動（前進，後退，右旋回，左旋回，握る，離す，止まる）\n",
    "        self.agent = Agent(self.num_states, self.num_actions)  # 環境内で行動するAgentを生成\n",
    "        '''pybullet'''\n",
    "        p.connect(p.GUI)\n",
    "        p.setAdditionalSearchPath(\"../../ros_ws/src/test_car_description/urdf/\")\n",
    "        self.maxForce = 10\n",
    "        \n",
    "    def renderPicture(self, height=320, width=320):\n",
    "        '''bullet側からカメラ画像を取得'''\n",
    "        base_pos, orn = p.getBasePositionAndOrientation(self.car)\n",
    "        cam_eye = np.array(base_pos) + [0.1,0,0.2]\n",
    "        cam_target = np.array(base_pos) + [2,0,0.2]\n",
    "        cam_upvec = [1,0,1]\n",
    "\n",
    "        view_matrix = p.computeViewMatrix(\n",
    "                cameraEyePosition=cam_eye,\n",
    "                cameraTargetPosition=cam_target,\n",
    "                cameraUpVector=cam_upvec)\n",
    "\n",
    "        proj_matrix = p.computeProjectionMatrixFOV(\n",
    "            fov=60, aspect=float(width)/height,\n",
    "            nearVal=0.1, farVal=100.0)\n",
    "\n",
    "        (_, _, rgb, _, mask) = p.getCameraImage(\n",
    "            width=width, height=height, viewMatrix=view_matrix,\n",
    "            projectionMatrix=proj_matrix, renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "        rgb_array = np.array(rgb)\n",
    "        rgb_array = rgb_array[:,:,:3]\n",
    "        mask_array = np.array(mask)\n",
    "\n",
    "        return rgb_array\n",
    "\n",
    "    def green_detect(self, img):\n",
    "        '''緑色のマスク'''\n",
    "        # HSV色空間に変換\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        # 緑色のHSVの値域\n",
    "        hsv_min = np.array([50, 100, 100])\n",
    "        hsv_max = np.array([70, 255, 255])\n",
    "        mask = cv2.inRange(hsv, hsv_min, hsv_max)\n",
    "        return mask\n",
    "\n",
    "    def calc_area(self, img):\n",
    "        '''面積計算'''\n",
    "        img = self.green_detect(img)\n",
    "        pix_area = cv2.countNonZero(img)  # ピクセル数\n",
    "        # パーセントを算出\n",
    "        h, w = img.shape  # frameの面積\n",
    "        per = round(100 * float(pix_area) / (w * h), 3)  # 0-100で規格化\n",
    "        print('GREEN_AREA: ', per)\n",
    "        return pix_area, per\n",
    "\n",
    "    def reset(self):\n",
    "        '''環境を初期化する'''\n",
    "        print('Environment.reset\\n')\n",
    "        \n",
    "        #bulletの世界をリセット\n",
    "        p.resetSimulation()\n",
    "        \n",
    "        #フィールドを表示\n",
    "        p.setGravity(0,0,-10)\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\")\n",
    "        \n",
    "        #オブジェクトモデルを表示\n",
    "        self.startPos = [0,0,0]\n",
    "        self.startOrientation = p.getQuaternionFromEuler([0,0,0])\n",
    "        self.car = p.loadURDF(\"test_car.urdf\", self.startPos, self.startOrientation)\n",
    "        \n",
    "        # ターゲットを表示\n",
    "        targetX, targetY = np.random.permutation(np.arange(10))[0:2]\n",
    "        self.targetPos = [targetX, targetY, 0]\n",
    "        self.target = p.createCollisionShape(\n",
    "            p.GEOM_CYLINDER, radius=0.2, height=2, collisionFramePosition=self.targetPos)\n",
    "        p.createMultiBody(0, self.target)\n",
    "\n",
    "        # 目標の面積, 重心の位置を取得する\n",
    "        frame = self.renderPicture()\n",
    "        _, area_sum = self.calc_area(frame)\n",
    "        area_v = 0\n",
    "\n",
    "        observation = (area_sum, area_v)\n",
    "        \n",
    "        return observation, frame\n",
    "\n",
    "    def get_env(self, area_sum_before):\n",
    "        '''環境を認識する'''\n",
    "        '''カメラで写真をとりOpencv2で面積と重心を取得する'''\n",
    "        frame = self.renderPicture()\n",
    "        # 赤色の面積とその変化量, 重心の位置とその変化量を取得する\n",
    "        _, area_sum = self.calc_area(frame)\n",
    "        area_v = area_sum - area_sum_before\n",
    "        # 観測量として返す\n",
    "        observation = (area_sum, area_v)\n",
    "        return observation, frame\n",
    "\n",
    "    def act_env(self, observation, action):\n",
    "        '''決定したactionに従って、ロボットハンドを動かす'''\n",
    "        if action == 0:  # 前\n",
    "            self.go()\n",
    "        elif action == 1:  # 後\n",
    "            self.back()\n",
    "        elif action == 2:  # 右\n",
    "            self.right()\n",
    "        elif action == 3:  # 左\n",
    "            self.left()\n",
    "        \n",
    "        for i in range(50):\n",
    "            p.stepSimulation()\n",
    "            time.sleep(1./240.)\n",
    "        \n",
    "        area_sum, _ = observation\n",
    "        observation_next, _ = self.get_env(area_sum)\n",
    "        done = self.is_done(observation_next)\n",
    "        \n",
    "        return observation_next, done\n",
    "\n",
    "    def go(self):\n",
    "        p.setJointMotorControlArray(\n",
    "                self.car, np.arange(p.getNumJoints(self.car))[1:], p.VELOCITY_CONTROL, \n",
    "                targetVelocities=[20,20,20,20],\n",
    "                forces=np.ones(4)*self.maxForce)\n",
    "    def back(self):\n",
    "        p.setJointMotorControlArray(\n",
    "                self.car, np.arange(p.getNumJoints(self.car))[1:], p.VELOCITY_CONTROL, \n",
    "                targetVelocities=[-20,-20,-20,-20],\n",
    "                forces=np.ones(4)*self.maxForce)\n",
    "    def right(self):\n",
    "        p.setJointMotorControlArray(\n",
    "                self.car, np.arange(p.getNumJoints(self.car))[1:], p.VELOCITY_CONTROL, \n",
    "                targetVelocities=[20,12,20,12],\n",
    "                forces=np.ones(4)*self.maxForce)\n",
    "    def left(self):\n",
    "        p.setJointMotorControlArray(\n",
    "                self.car, np.arange(p.getNumJoints(self.car))[1:], p.VELOCITY_CONTROL, \n",
    "                targetVelocities=[12,20,12,20],\n",
    "                forces=np.ones(4)*self.maxForce)\n",
    "    def stop(self):\n",
    "        p.setJointMotorControlArray(\n",
    "                self.car, np.arange(p.getNumJoints(self.car))[1:], p.VELOCITY_CONTROL, \n",
    "                targetVelocities=[0,0,0,0],\n",
    "                forces=np.ones(4)*self.maxForce)\n",
    "    \n",
    "    def is_done(self, observation):\n",
    "        '''observationによって終了判定をする'''\n",
    "        #終了判定は面積が閾値以上&面積の変化なし（重心位置が画像の真ん中？カメラの位置によるけど，とりあえずはなし）\n",
    "        done = False\n",
    "        area_sum, area_v = observation\n",
    "        if area_sum > AREA_THRESH:\n",
    "            done = True\n",
    "        return done\n",
    "\n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        print('Environment.run')\n",
    "        complete_episodes = 0  # 連続で取り続けた試行数\n",
    "        is_episode_final = False  # 最終試行フラグ\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n",
    "            observation, frame = self.reset()  # 環境の初期化\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "                print('Step: {0} of Episode: {1}'.format(step+1, episode))\n",
    "                # 行動を求める\n",
    "                action = self.agent.get_action(observation, episode)\n",
    "                # 行動a_tの実行により、s_{t+1}, r_{t+1}を求める\n",
    "                observation_next, done = self.act_env(observation, action)\n",
    "\n",
    "                # 報酬を与える\n",
    "                if done:\n",
    "                    reward = 1  # 目標を掴んだら報酬1を与える\n",
    "                    print ('reward: +1')\n",
    "                    complete_episodes += 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = 0  # 途中の報酬は0\n",
    "                    print ('reward: 0')\n",
    "\n",
    "                # step+1の状態observation_nextを用いて,Q関数を更新する\n",
    "                if TEST_MODE:  # 保存したQ-TABLEを使用する\n",
    "                    continue\n",
    "                else:\n",
    "                    self.agent.update_Q_function(observation, action, reward, observation_next)\n",
    "\n",
    "                # 観測の更新\n",
    "                observation = observation_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('{0} Episode: Finished after {1} time steps'.format(episode, step + 1))\n",
    "                    break\n",
    "\n",
    "                # 1episode内でdoneできなかったら罰を与える\n",
    "                if step == MAX_STEPS-1:\n",
    "                    reward = -1\n",
    "                    print('reward: -1')\n",
    "                    complete_episodes = 0  # 4step以上連続で立ち続けた試行数をリセット\n",
    "\n",
    "            if is_episode_final is True:  # 最終試行では動画を保存と描画\n",
    "                Brain(num_states=self.num_states, num_actions=self.num_actions).save_Q_table()  # Q-tableを保存する\n",
    "                print('finished')\n",
    "                break\n",
    "\n",
    "            if complete_episodes >= 10:  # 5連続成功なら\n",
    "                print('10回連続成功\\n次で最終試行')\n",
    "                is_episode_final = True  # 次の試行を最終試行とする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment.run\n",
      "Environment.reset\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Cannot load URDF file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5859dbcee31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrobot_hand_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrobot_hand_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-686c8f61dce3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 試行数分繰り返す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 環境の初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1エピソードのループ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-686c8f61dce3>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#フィールドを表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetGravity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaneId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadURDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plane.urdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#オブジェクトモデルを表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Cannot load URDF file."
     ]
    }
   ],
   "source": [
    "robot_hand_env = Environment()\n",
    "robot_hand_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dl-box/atsushi/github/pybullet/robot_hand_QL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
